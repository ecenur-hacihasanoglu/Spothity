{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f4f9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n# ╔══════════════════════════════════════════════════════════════╗\\n#   SPLIT merged_data.csv → smaller CSV files (memory-safe)      #\\n# ╚══════════════════════════════════════════════════════════════╝\\n\\nimport pandas as pd, pathlib, time\\nfrom tqdm.notebook import tqdm\\n\\n# ------- 1 | paths & parameters ---------------------------------------\\nCSV_PATH   = pathlib.Path(\"./merged_data.csv\")    # 25 GB source\\nOUT_DIR    = pathlib.Path(\"csv_chunks\")           # output folder\\nOUT_DIR.mkdir(exist_ok=True)\\n\\nSAMPLE_ROWS = 100_000     # to estimate average row size\\nTARGET_MB   = 300         # aim ≈ this much RAM per chunk\\nBYTES_MB    = 1_048_576\\n\\n# (optional) limit columns or set dtypes here to shrink memory\\nUSECOLS  = None           # e.g. [\"track_id\",\"title\",\"artist\", ...]\\nDTYPES   = None           # e.g. {\"streams\":\"float32\", ...}\\n\\n# ------- 2 | estimate rows per chunk -----------------------------------\\nt0 = time.time()\\nsample = pd.read_csv(\\n    CSV_PATH,\\n    nrows=SAMPLE_ROWS,\\n    usecols=USECOLS,\\n    dtype=DTYPES,\\n    low_memory=True,\\n    encoding_errors=\"ignore\"\\n)\\nrow_bytes = sample.memory_usage(deep=True).sum() / SAMPLE_ROWS\\nROWS_PER_CHUNK = int((TARGET_MB * BYTES_MB) / row_bytes)\\ndel sample\\n\\nprint(f\"► Target RAM~{TARGET_MB} MB → {ROWS_PER_CHUNK:,} rows/chunk \"\\n      f\"(est. {row_bytes/1024:.1f} KB per row, calc {time.time()-t0:.1f}s)\")\\n\\n# ------- 3 | stream-read & write chunks --------------------------------\\nreader = pd.read_csv(\\n    CSV_PATH,\\n    chunksize=ROWS_PER_CHUNK,\\n    usecols=USECOLS,\\n    dtype=DTYPES,\\n    low_memory=True,\\n    encoding_errors=\"ignore\"\\n)\\n\\nfor i, chunk in enumerate(tqdm(reader, desc=\"Splitting CSV\"), 1):\\n    out_file = OUT_DIR / f\"part_{i:03}.csv\"\\n    chunk.to_csv(out_file, index=False)\\n    print(f\"✓ {out_file.name:>10} · {len(chunk):,} rows\")\\n    del chunk                     # free RAM\\n\\nprint(\"🎉  Done.  All parts saved in\", OUT_DIR)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "#   SPLIT merged_data.csv → smaller CSV files (memory-safe)      #\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "\n",
    "import pandas as pd, pathlib, time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ------- 1 | paths & parameters ---------------------------------------\n",
    "CSV_PATH   = pathlib.Path(\"./merged_data.csv\")    # 25 GB source\n",
    "OUT_DIR    = pathlib.Path(\"csv_chunks\")           # output folder\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SAMPLE_ROWS = 100_000     # to estimate average row size\n",
    "TARGET_MB   = 300         # aim ≈ this much RAM per chunk\n",
    "BYTES_MB    = 1_048_576\n",
    "\n",
    "# (optional) limit columns or set dtypes here to shrink memory\n",
    "USECOLS  = None           # e.g. [\"track_id\",\"title\",\"artist\", ...]\n",
    "DTYPES   = None           # e.g. {\"streams\":\"float32\", ...}\n",
    "\n",
    "# ------- 2 | estimate rows per chunk -----------------------------------\n",
    "t0 = time.time()\n",
    "sample = pd.read_csv(\n",
    "    CSV_PATH,\n",
    "    nrows=SAMPLE_ROWS,\n",
    "    usecols=USECOLS,\n",
    "    dtype=DTYPES,\n",
    "    low_memory=True,\n",
    "    encoding_errors=\"ignore\"\n",
    ")\n",
    "row_bytes = sample.memory_usage(deep=True).sum() / SAMPLE_ROWS\n",
    "ROWS_PER_CHUNK = int((TARGET_MB * BYTES_MB) / row_bytes)\n",
    "del sample\n",
    "\n",
    "print(f\"► Target RAM~{TARGET_MB} MB → {ROWS_PER_CHUNK:,} rows/chunk \"\n",
    "      f\"(est. {row_bytes/1024:.1f} KB per row, calc {time.time()-t0:.1f}s)\")\n",
    "\n",
    "# ------- 3 | stream-read & write chunks --------------------------------\n",
    "reader = pd.read_csv(\n",
    "    CSV_PATH,\n",
    "    chunksize=ROWS_PER_CHUNK,\n",
    "    usecols=USECOLS,\n",
    "    dtype=DTYPES,\n",
    "    low_memory=True,\n",
    "    encoding_errors=\"ignore\"\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(tqdm(reader, desc=\"Splitting CSV\"), 1):\n",
    "    out_file = OUT_DIR / f\"part_{i:03}.csv\"\n",
    "    chunk.to_csv(out_file, index=False)\n",
    "    print(f\"✓ {out_file.name:>10} · {len(chunk):,} rows\")\n",
    "    del chunk                     # free RAM\n",
    "\n",
    "print(\"🎉  Done.  All parts saved in\", OUT_DIR)\n",
    "\"\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d73955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPOTIFY_CLIENT_ID = \"32be97271df4443fa4b9cff2a8255a7d\"\n",
    "SPOTIFY_CLIENT_SECRET = \"99205f2f007640ff9089fd55f23b035e\"\n",
    "GENIUS_ACCESS_TOKEN = \"QqxgssaR8kzXUObcrgsbCgKmlWagwwvO-V8Y3yYgU9r1vAeOHzHJGPh3EHbg31ul\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fca42e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 7,612 rows read, 6,977 rows after deduplication\n"
     ]
    }
   ],
   "source": [
    "# ╔═════════ 0 | dedupe your old Excel by title+artist ════════════════════\n",
    "import pandas as pd\n",
    "\n",
    "# 1) read your old Excel (adjust path & sheet_name as needed)\n",
    "old_df = pd.read_excel(\"1507.xlsx\", sheet_name=\"Sheet1\",\n",
    "                       dtype={\"artist\":\"string\",\"title\":\"string\",\"track_id\":\"string\",\"region\":\"string\"})\n",
    "def clean_title(title: str) -> str:\n",
    "    \"\"\"Drop everything in parentheses and trim.\"\"\"\n",
    "    return re.sub(r\"\\s*\\([^)]*\\)\", \"\", title).strip()\n",
    "def main_artist(artist: str) -> str:\n",
    "    \"\"\"Take only the first (primary) artist before a comma.\"\"\"\n",
    "    return artist.split(\",\")[0].strip()\n",
    "# 2) normalize for deduplication\n",
    "old_df[\"clean_title\"]  = old_df[\"title\"].apply(clean_title).str.lower()\n",
    "old_df[\"primary_artist\"] = old_df[\"artist\"].apply(main_artist).str.lower()\n",
    "\n",
    "# 3) drop duplicates on (clean_title, primary_artist), keeping the first occurrence\n",
    "deduped = old_df.drop_duplicates(subset=[\"clean_title\",\"primary_artist\"]).copy()\n",
    "\n",
    "# 4) (optional) drop the helper cols, then save out\n",
    "deduped = deduped.drop(columns=[\"clean_title\",\"primary_artist\"])\n",
    "deduped.to_csv(\"./deduped_old_tracks.csv\", index=False)\n",
    "\n",
    "print(f\"✅ {len(old_df):,} rows read, {len(deduped):,} rows after deduplication\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2daa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped.to_excel(\"./deduped_old_tracks.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537c1a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3d83ee87194ff1b5da99fd31a94c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "files:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- part_001.csv (12,279 US rows) ---\n",
      "\n",
      "--- part_002.csv (10,905 US rows) ---\n",
      "\n",
      "--- part_003.csv (13,402 US rows) ---\n",
      "\n",
      "--- part_004.csv (13,445 US rows) ---\n",
      "\n",
      "--- part_005.csv (13,793 US rows) ---\n",
      "\n",
      "--- part_006.csv (12,618 US rows) ---\n",
      "\n",
      "--- part_007.csv (14,262 US rows) ---\n",
      "\n",
      "--- part_008.csv (12,433 US rows) ---\n",
      "\n",
      "--- part_009.csv (13,274 US rows) ---\n",
      "\n",
      "--- part_010.csv (13,352 US rows) ---\n",
      "\n",
      "--- part_011.csv (14,183 US rows) ---\n",
      "\n",
      "--- part_012.csv (13,047 US rows) ---\n",
      "\n",
      "--- part_013.csv (13,590 US rows) ---\n",
      "\n",
      "--- part_014.csv (13,924 US rows) ---\n",
      "\n",
      "--- part_015.csv (14,498 US rows) ---\n",
      "\n",
      "--- part_016.csv (14,877 US rows) ---\n",
      "\n",
      "--- part_017.csv (14,641 US rows) ---\n",
      "\n",
      "--- part_018.csv (13,115 US rows) ---\n",
      "\n",
      "--- part_019.csv (14,744 US rows) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 165\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m US rows) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m TARGET_KEEP:\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget reached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\4444e\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\base.py:497\u001b[0m, in \u001b[0;36mExtensionArray.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# This needs to be implemented so that pandas recognizes extension\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# arrays as list-like. The default implementation makes successive\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# calls to ``__getitem__``, which may be slower than necessary.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m[i]\n",
      "File \u001b[1;32mc:\\Users\\4444e\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:282\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     key: PositionalIndexer2D,\n\u001b[0;32m    281\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self \u001b[38;5;241m|\u001b[39m Any:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_integer(key):\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;66;03m# fast-path\u001b[39;00m\n\u001b[0;32m    284\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndarray[key]\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ╔═════════ 1 | imports & folders ═════════════════════════════════════ \n",
    "import pandas as pd, pathlib, json, time, re\n",
    "from spotipy import Spotify\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import lyricsgenius\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "IN_DIR    = pathlib.Path(\"csv_chunks\")\n",
    "OUT_DIR   = pathlib.Path(\"csv_enriched\");  OUT_DIR.mkdir(exist_ok=True)\n",
    "CACHE_DIR = pathlib.Path(\".cache_enrich\"); CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TARGET_KEEP = 15000           # final row count\n",
    "\n",
    "# ╔═════════ 2 | API sessions & caches ═════════════════════════════════\n",
    "sp = Spotify(auth_manager=SpotifyClientCredentials(\n",
    "    client_id=SPOTIFY_CLIENT_ID, client_secret=SPOTIFY_CLIENT_SECRET))\n",
    "\n",
    "genius = lyricsgenius.Genius(\n",
    "    GENIUS_ACCESS_TOKEN, timeout=15, retries=3,\n",
    "    skip_non_songs=True, remove_section_headers=True, verbose=False)\n",
    "\n",
    "def _load(n):\n",
    "    p = CACHE_DIR / f\"{n}.json\"\n",
    "    return json.loads(p.read_text()) if p.exists() else {}\n",
    "\n",
    "def _save(n, o):\n",
    "    (CACHE_DIR / f\"{n}.json\").write_text(json.dumps(o))\n",
    "\n",
    "lyrics_cache    = _load(\"lyrics\")        # {track_id: lyrics or \"\"}\n",
    "artist_cache    = _load(\"artist_ids\")    # {track_id: artist_id}\n",
    "followers_cache = _load(\"followers\")     # {artist_id: followers}\n",
    "\n",
    "# ╔═════════ 3 | helpers  (lyrics & followers) ═════════════════════════\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2))\n",
    "def g_search(title, artist):\n",
    "    return genius.search_song(title=title, artist=artist)\n",
    "\n",
    "def clean_title(title: str) -> str:\n",
    "    \"\"\"Drop everything in parentheses and trim.\"\"\"\n",
    "    return re.sub(r\"\\s*\\([^)]*\\)\", \"\", title).strip()\n",
    "\n",
    "def main_artist(artist: str) -> str:\n",
    "    \"\"\"Take only the first (primary) artist before a comma.\"\"\"\n",
    "    return artist.split(\",\")[0].strip()\n",
    "\n",
    "def get_lyrics(tid: str, title: str, artist: str) -> str:\n",
    "    if tid in lyrics_cache:\n",
    "        return lyrics_cache[tid]\n",
    "\n",
    "    raw_title = clean_title(title)\n",
    "    primary   = main_artist(artist)\n",
    "    print(f\"♪ Searching: {raw_title} – {primary}\", end=\"\")\n",
    "\n",
    "    try:\n",
    "        # bias toward original page with the word lyrics\n",
    "        song = g_search(f\"{raw_title}\", primary+\" english\")\n",
    "\n",
    "        if (\n",
    "            not song or\n",
    "            song.primary_artist.name.lower() != primary.lower() or\n",
    "            any(k in song.title.lower() for k in (\"translation\", \"çeviri\", \"traducción\"))\n",
    "        ):\n",
    "            song = None\n",
    "\n",
    "        lyrics_cache[tid] = song.lyrics[:4000] if song and song.lyrics else \"\"\n",
    "        print(\"  →  FOUND with english\" if lyrics_cache[tid] else \"  →  NOT FOUND with english\")\n",
    "        if not lyrics_cache[tid]:\n",
    "            song = g_search(f\"{raw_title}\", primary+\" lyrics\")\n",
    "\n",
    "            if (\n",
    "            not song or\n",
    "            song.primary_artist.name.lower() != primary.lower() or\n",
    "            any(k in song.title.lower() for k in (\"translation\", \"çeviri\", \"traducción\"))\n",
    "            ):\n",
    "                song = None\n",
    "\n",
    "            lyrics_cache[tid] = song.lyrics[:4000] if song and song.lyrics else \"\"\n",
    "            print(\"  →  FOUND with lyrics\" if lyrics_cache[tid] else \"  →  NOT FOUND with lyrics\")\n",
    "            if not lyrics_cache[tid]:\n",
    "                \n",
    "                song = g_search(f\"{primary}\", raw_title+\" english\")\n",
    "                if (\n",
    "                not song or\n",
    "                song.primary_artist.name.lower() != primary.lower() or\n",
    "                any(k in song.title.lower() for k in (\"translation\", \"çeviri\", \"traducción\"))\n",
    "                ):\n",
    "                    song = None\n",
    "\n",
    "                lyrics_cache[tid] = song.lyrics[:4000] if song and song.lyrics else \"\"\n",
    "                print(\"  →  FOUND with english backwards\" if lyrics_cache[tid] else \"  →  NOT FOUND with english backwards\")\n",
    "                if not lyrics_cache[tid]:\n",
    "                    song = g_search(f\"{primary}\", raw_title+\" lyrics\")\n",
    "                    if (\n",
    "                    not song or\n",
    "                    song.primary_artist.name.lower() != primary.lower() or\n",
    "                    any(k in song.title.lower() for k in (\"çeviri\", \"traducción\"))\n",
    "                    ):\n",
    "                        song = None\n",
    "\n",
    "                    lyrics_cache[tid] = song.lyrics[:4000] if song and song.lyrics else \"\"\n",
    "                    print(\"  →  FOUND with lyrics backwards\" if lyrics_cache[tid] else \"  →  NOT FOUND with lyrics backwards\")\n",
    "                if not lyrics_cache[tid]:\n",
    "                    song = g_search(f\"{primary}\", raw_title)\n",
    "                    if (\n",
    "                    not song or\n",
    "                    song.primary_artist.name.lower() != primary.lower() or\n",
    "                    any(k in song.title.lower() for k in (\"çeviri\", \"traducción\"))\n",
    "                    ):\n",
    "                        song = None\n",
    "\n",
    "                    lyrics_cache[tid] = song.lyrics[:4000] if song and song.lyrics else \"\"\n",
    "                    print(\"  →  FOUND with raw\" if lyrics_cache[tid] else \"  →  NOT FOUND with raw\")                                    \n",
    "                \n",
    "            \n",
    "    except Exception as e:\n",
    "        lyrics_cache[tid] = \"\"\n",
    "        print(f\"  →  NOT FOUND ({e})\")\n",
    "            \n",
    "    _save(\"lyrics\", lyrics_cache)\n",
    "    time.sleep(0.7)            # Genius rate-limit\n",
    "    return lyrics_cache[tid]   \n",
    "\n",
    "def get_followers(tid: str) -> int:\n",
    "    if tid not in artist_cache:\n",
    "        try:\n",
    "            artist_cache[tid] = sp.track(tid)[\"artists\"][0][\"id\"]\n",
    "            _save(\"artist_ids\", artist_cache)\n",
    "        except Exception:\n",
    "            artist_cache[tid] = \"\"\n",
    "    aid = artist_cache[tid]\n",
    "\n",
    "    if aid and aid not in followers_cache:\n",
    "        try:\n",
    "            followers_cache[aid] = sp.artist(aid)[\"followers\"][\"total\"]\n",
    "            _save(\"followers\", followers_cache)\n",
    "            time.sleep(0.2)\n",
    "        except Exception:\n",
    "            followers_cache[aid] = 0\n",
    "    return followers_cache.get(aid, 0)\n",
    "\n",
    "# ╔═════════ 4 | main loop (US only, dedupe by title+artist) ═════════════\n",
    "kept, seen = [], set()  \n",
    "# seen: set of (cleaned_title.lower(), primary_artist.lower())\n",
    "\n",
    "for src in tqdm(sorted(IN_DIR.glob(\"part_*.csv\")), desc=\"files\"):\n",
    "    if len(kept) >= TARGET_KEEP:\n",
    "        print( \"target reached\")\n",
    "        break\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        src,\n",
    "        dtype={\"artist\":\"string\",\"title\":\"string\",\"track_id\":\"string\",\"region\":\"string\"},\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    valid_regions = [\"United States\", \"Canada\", \"United Kingdom\", \"Australia\"]\n",
    "    df = df[df[\"region\"].isin(valid_regions) & (df[\"popularity\"] > 0)]\n",
    "    if df.empty:\n",
    "        print(\"df is empty\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- {src.name} ({len(df):,} US rows) ---\")\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        if len(kept) >= TARGET_KEEP:\n",
    "            print( \"target reached\")\n",
    "            break\n",
    "\n",
    "        title, artist = row.title, row.artist\n",
    "        if pd.isna(title) or pd.isna(artist):\n",
    "            continue\n",
    "\n",
    "        # build dedupe key\n",
    "        title_key  = clean_title(title).lower()\n",
    "        artist_key = main_artist(artist).lower()\n",
    "        song_key   = (title_key, artist_key)\n",
    "\n",
    "        # skip if we've already kept this song (same title+artist)\n",
    "        if song_key in seen:\n",
    "        #    print(\"seen song\")\n",
    "            continue\n",
    "\n",
    "        # fetch lyrics & followers\n",
    "        tid  = row.track_id\n",
    "        lyr  = get_lyrics(tid, title, artist)\n",
    "        if not lyr:\n",
    "            continue\n",
    "\n",
    "        foll = get_followers(tid)\n",
    "        if foll == 0:\n",
    "            continue\n",
    "\n",
    "        kept.append({**row._asdict(), \"lyrics\": lyr, \"artist_followers\": foll})\n",
    "        seen.add(song_key)\n",
    "\n",
    "print(f\"\\n✅ collected {len(kept):,} unique songs (by title+artist) with lyrics & followers\")\n",
    "\n",
    "# ╔═════════ 5 | save result ═══════════════════════════════════════════\n",
    "pd.DataFrame(kept).to_csv(OUT_DIR / \"sample_15000.csv\", index=False)\n",
    "print(\"📄 saved csv_enriched/sample_US_15000.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df19bb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7612"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(kept)[\"track_id\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85958ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────\n",
    "# 0. KÜTÜPHANELER\n",
    "# ──────────────────────────────\n",
    "from transformers import pipeline\n",
    "import torch, pandas as pd, requests, time, re, string\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# ──────────────────────────────\n",
    "# 1. GoEmotions MODEL\n",
    "# ──────────────────────────────\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "emo_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"SamLowe/roberta-base-go_emotions\",\n",
    "    top_k=None,\n",
    "    truncation=True,\n",
    "    device=device,\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "positive_tags = {\"joy\", \"love\", \"excitement\", \"admiration\"}\n",
    "negative_tags = {\"sadness\", \"anger\", \"fear\"}\n",
    "relevant_tags = sorted(list(positive_tags | negative_tags | {\"neutral\"}))\n",
    "\n",
    "\n",
    "# ──────────────────────────────\n",
    "# 3. LDA (scikit-learn) YARDIMCI\n",
    "# ──────────────────────────────\n",
    "def sklearn_topics(series, n_topics):\n",
    "    \"\"\"Verilen string Series için LDA -> en olası topic id dizisi döndürür.\"\"\"\n",
    "    vect = CountVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        token_pattern=r\"\\b[a-zA-Z]{4,}\\b\"   # ≥4 harfli kelimeler\n",
    "    )\n",
    "    X = vect.fit_transform(series.fillna(\"\"))\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=10,\n",
    "        learning_method=\"online\",\n",
    "        random_state=42\n",
    "    ).fit(X)\n",
    "    topic_ids = lda.transform(X).argmax(axis=1)  # her belge için argmax\n",
    "    return topic_ids\n",
    "\n",
    "# ──────────────────────────────\n",
    "# 4. ANA PIPELINE\n",
    "# ──────────────────────────────\n",
    "def add_all_features(df, max_len=2000):\n",
    "    df = df.copy()\n",
    "    df[\"title_topic\"]  = sklearn_topics(df[\"title\"], n_topics=10)\n",
    "    df[\"lyrics_topic\"] = sklearn_topics(df[\"lyrics\"], n_topics=20)\n",
    "\n",
    "    emo_scores = {f\"emo_{t}\":[] for t in relevant_tags}\n",
    "    g_label, g_score = [], []\n",
    "    pos_list, neg_list, emo_int = [], [], []\n",
    "\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"GoEmotions + Deezer + LDA\"):\n",
    "        # ---- GoEmotions ----\n",
    "        snippet = (row.get(\"lyrics\") or \"\")[:max_len]\n",
    "        pred = emo_pipe(snippet)[0] if snippet else [{\"label\":\"neutral\",\"score\":1.0}]\n",
    "        scores = {d[\"label\"]:d[\"score\"] for d in pred}\n",
    "\n",
    "        pos = sum(scores.get(t,0) for t in positive_tags)\n",
    "        neg = sum(scores.get(t,0) for t in negative_tags)\n",
    "        neu = scores.get(\"neutral\",0)\n",
    "        emo_sum = pos + neg\n",
    "\n",
    "        if max(pos,neg,neu) == pos:\n",
    "            g_label.append(\"POSITIVE\"); g_score.append(pos)\n",
    "        elif max(pos,neg,neu) == neg:\n",
    "            g_label.append(\"NEGATIVE\"); g_score.append(neg)\n",
    "        else:\n",
    "            g_label.append(\"NEUTRAL\");  g_score.append(neu)\n",
    "\n",
    "        pos_list.append(pos)\n",
    "        neg_list.append(neg)\n",
    "        emo_int.append(emo_sum)\n",
    "\n",
    "        for t in relevant_tags:\n",
    "            emo_scores[f\"emo_{t}\"].append(scores.get(t,0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df[\"bert_sent_label\"]      = g_label\n",
    "    df[\"bert_sent_score\"]      = g_score\n",
    "    df[\"positivity\"]           = pos_list\n",
    "    df[\"negativity\"]           = neg_list\n",
    "    df[\"emotional_intensity\"]  = emo_int\n",
    "\n",
    "\n",
    "    for t in relevant_tags:\n",
    "        df[f\"bert_{t}\"] = emo_scores[f\"emo_{t}\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28bdf267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9e685d98864adb86f0fdc26d3376c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GoEmotions + Deezer + LDA:   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_features = add_all_features(pd.DataFrame(kept))\n",
    "df_features = df_features[df_features['lyrics'].notnull() & (df_features['lyrics'].str.strip() != '')]\n",
    "df_features.to_excel(\"1507.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff0e770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0       5aAx2yezTd8zXrkmtKl66Z\n",
       "1       5knuzwU65gJK7IF5yJsuaW\n",
       "2       7BKLCZ1jbUBVqRi2FVlTVw\n",
       "3       78rIJddV4X0HkNAInEcYde\n",
       "4       5uCax9HTNlzGybIStD3vDh\n",
       "                 ...          \n",
       "7607    5oyMCoYK2rRMjAvZUHbn7R\n",
       "7608    1QO9k34FcHXGqdMUWjH1Rn\n",
       "7609    0Sc5lXL9iqUHibXRfiquGn\n",
       "7610    0y7Xf4d6snUqmMFrCTD3ql\n",
       "7611    3YKAaNsQ9i5oiSPeDw5PxV\n",
       "Name: track_id, Length: 7612, dtype: object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[\"track_id\"].unique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
